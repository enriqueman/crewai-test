import re
import json
from typing import Dict, List, Any, Optional
from crewai_tools import BaseTool
from pydantic import BaseModel, Field
from collections import Counter
import nltk

# Descargar recursos de NLTK si no est√°n disponibles
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    pass  # En Lambda no podemos descargar, usaremos regex

class ContentAnalysisInput(BaseModel):
    """Input para la herramienta de an√°lisis de contenido"""
    content: str = Field(..., description="Contenido a analizar")
    analysis_type: str = Field(default="comprehensive", description="Tipo de an√°lisis: comprehensive, keywords, readability, structure")

class ContentAnalyzerTool(BaseTool):
    name: str = "Content Analyzer Tool"
    description: str = """
    Herramienta avanzada para analizar contenido de marketing. Puede identificar:
    - Keywords y temas principales
    - M√©tricas de legibilidad
    - Estructura del contenido
    - Sentiment y tone
    - Gaps de contenido
    - Optimizaci√≥n SEO
    """
    args_schema: type[BaseModel] = ContentAnalysisInput
    
    def _run(self, content: str, analysis_type: str = "comprehensive") -> str:
        """Ejecuta el an√°lisis de contenido"""
        try:
            if analysis_type == "keywords":
                return self._analyze_keywords(content)
            elif analysis_type == "readability":
                return self._analyze_readability(content)
            elif analysis_type == "structure":
                return self._analyze_structure(content)
            elif analysis_type == "seo":
                return self._analyze_seo(content)
            else:
                return self._comprehensive_analysis(content)
                
        except Exception as e:
            return f"Error en an√°lisis de contenido: {str(e)}"
    
    def _comprehensive_analysis(self, content: str) -> str:
        """An√°lisis completo del contenido"""
        results = {
            "content_metrics": self._get_content_metrics(content),
            "keyword_analysis": self._extract_keywords(content),
            "structure_analysis": self._analyze_content_structure(content),
            "readability": self._calculate_readability(content),
            "seo_metrics": self._analyze_seo_factors(content),
            "content_gaps": self._identify_content_gaps(content),
            "recommendations": self._generate_recommendations(content)
        }
        
        return self._format_analysis_results(results)
    
    def _get_content_metrics(self, content: str) -> Dict[str, Any]:
        """Obtiene m√©tricas b√°sicas del contenido"""
        words = len(content.split())
        sentences = len(re.split(r'[.!?]+', content))
        paragraphs = len([p for p in content.split('\n\n') if p.strip()])
        
        return {
            "word_count": words,
            "sentence_count": sentences,
            "paragraph_count": paragraphs,
            "average_words_per_sentence": round(words / max(sentences, 1), 2),
            "average_sentences_per_paragraph": round(sentences / max(paragraphs, 1), 2),
            "character_count": len(content),
            "character_count_no_spaces": len(content.replace(" ", ""))
        }
    
    def _extract_keywords(self, content: str) -> Dict[str, Any]:
        """Extrae y analiza keywords del contenido"""
        # Limpiar contenido
        clean_content = re.sub(r'[^\w\s]', '', content.lower())
        words = clean_content.split()
        
        # Filtrar stop words b√°sicas
        stop_words = {
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'del', 'los', 'las', 'una', 'como', 'pero', 'sus', 'han', 'muy', 'est√°', 'son', 'todo', 'esta', 'm√°s', 'tiene', 'si', 'ya', 'puede', 'bien', 'hacer', 'sobre', 'ser', 'era', 'vez', 'solo', 'desde', 'cada', 'hasta', 'tambi√©n', 'otros', 'donde', 'cuando', 'mismo', 'tanto', 'algo', 'qu√©', 'porque', 'as√≠', 'c√≥mo', 'tanto',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must', 'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their', 'we', 'us', 'our', 'you', 'your', 'i', 'me', 'my', 'he', 'him', 'his', 'she', 'her', 'hers'
        }
        
        # Filtrar palabras relevantes
        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Contar frecuencias
        word_freq = Counter(filtered_words)
        
        # Keywords relacionados con marketing
        marketing_keywords = [
            'content', 'marketing', 'digital', 'social', 'media', 'seo', 'strategy', 'brand', 'engagement', 'analytics', 'roi', 'conversion', 'automation', 'personalization', 'ai', 'artificial', 'intelligence', 'video', 'blog', 'email', 'influencer', 'campaign', 'audience', 'target', 'customer', 'user', 'experience', 'data', 'insights', 'trends', 'innovation', 'growth', 'optimization', 'performance', 'metrics', 'kpi'
        ]
        
        # Identificar keywords de marketing en el contenido
        found_marketing_keywords = {kw: word_freq.get(kw, 0) for kw in marketing_keywords if kw in word_freq}
        
        return {
            "total_unique_words": len(set(words)),
            "top_keywords": dict(word_freq.most_common(20)),
            "marketing_keywords_found": found_marketing_keywords,
            "keyword_density": round(len(filtered_words) / len(words) * 100, 2) if words else 0
        }
    
    def _analyze_content_structure(self, content: str) -> Dict[str, Any]:
        """Analiza la estructura del contenido"""
        # Detectar headers
        headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
        
        # Detectar listas
        bullet_points = len(re.findall(r'^\s*[-*‚Ä¢]\s+', content, re.MULTILINE))
        numbered_lists = len(re.findall(r'^\s*\d+\.\s+', content, re.MULTILINE))
        
        # Detectar enlaces y referencias
        links = len(re.findall(r'https?://\S+', content))
        email_addresses = len(re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', content))
        
        # Detectar formatting
        bold_text = len(re.findall(r'\*\*(.+?)\*\*', content))
        italic_text = len(re.findall(r'\*(.+?)\*', content))
        
        return {
            "headers_count": len(headers),
            "headers": headers[:10],  # Solo los primeros 10
            "bullet_points": bullet_points,
            "numbered_lists": numbered_lists,
            "links_count": links,
            "email_addresses": email_addresses,
            "bold_formatting": bold_text,
            "italic_formatting": italic_text,
            "has_good_structure": len(headers) >= 3 and (bullet_points > 0 or numbered_lists > 0)
        }
    
    def _calculate_readability(self, content: str) -> Dict[str, Any]:
        """Calcula m√©tricas de legibilidad"""
        sentences = re.split(r'[.!?]+', content)
        words = content.split()
        
        if not sentences or not words:
            return {"error": "Contenido insuficiente para an√°lisis"}
        
        # Flesch Reading Ease (aproximado)
        avg_sentence_length = len(words) / len(sentences)
        
        # Contar s√≠labas aproximadamente
        syllables = sum([self._count_syllables(word) for word in words])
        avg_syllables_per_word = syllables / len(words) if words else 0
        
        # Flesch Reading Ease Score (aproximado)
        flesch_score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables_per_word)
        
        # Interpretar score
        if flesch_score >= 90:
            reading_level = "Muy f√°cil"
        elif flesch_score >= 80:
            reading_level = "F√°cil"
        elif flesch_score >= 70:
            reading_level = "Bastante f√°cil"
        elif flesch_score >= 60:
            reading_level = "Est√°ndar"
        elif flesch_score >= 50:
            reading_level = "Bastante dif√≠cil"
        elif flesch_score >= 30:
            reading_level = "Dif√≠cil"
        else:
            reading_level = "Muy dif√≠cil"
        
        return {
            "flesch_score": round(flesch_score, 2),
            "reading_level": reading_level,
            "avg_sentence_length": round(avg_sentence_length, 2),
            "avg_syllables_per_word": round(avg_syllables_per_word, 2),
            "estimated_reading_time_minutes": round(len(words) / 200, 1)  # 200 WPM promedio
        }
    
    def _count_syllables(self, word: str) -> int:
        """Cuenta s√≠labas aproximadamente"""
        word = word.lower()
        vowels = "aeiouy"
        syllable_count = 0
        previous_was_vowel = False
        
        for char in word:
            if char in vowels:
                if not previous_was_vowel:
                    syllable_count += 1
                previous_was_vowel = True
            else:
                previous_was_vowel = False
        
        # Ajustes
        if word.endswith("e"):
            syllable_count -= 1
        if syllable_count == 0:
            syllable_count = 1
            
        return syllable_count
    
    def _analyze_seo_factors(self, content: str) -> Dict[str, Any]:
        """Analiza factores SEO del contenido"""
        word_count = len(content.split())
        
        # Detectar keywords en t√≠tulos
        headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
        
        # Meta informaci√≥n
        has_intro = len(content.split('\n\n')[0].split()) > 50 if content else False
        has_conclusion = "conclusi√≥n" in content.lower() or "resumen" in content.lower()
        
        # Densidad de keywords (aproximada)
        marketing_terms = ['marketing', 'content', 'digital', 'strategy', 'seo']
        keyword_mentions = sum([content.lower().count(term) for term in marketing_terms])
        keyword_density = (keyword_mentions / word_count * 100) if word_count > 0 else 0
        
        return {
            "word_count": word_count,
            "optimal_length": 1500 <= word_count <= 3000,
            "headers_count": len(headers),
            "has_intro": has_intro,
            "has_conclusion": has_conclusion,
            "keyword_density": round(keyword_density, 2),
            "seo_score": self._calculate_seo_score(word_count, len(headers), has_intro, has_conclusion, keyword_density)
        }
    
    def _calculate_seo_score(self, word_count: int, headers: int, has_intro: bool, has_conclusion: bool, keyword_density: float) -> int:
        """Calcula un score SEO b√°sico"""
        score = 0
        
        # Longitud del contenido
        if 1500 <= word_count <= 3000:
            score += 25
        elif word_count >= 1000:
            score += 15
        
        # Estructura
        if headers >= 3:
            score += 20
        elif headers >= 1:
            score += 10
        
        # Introducci√≥n y conclusi√≥n
        if has_intro:
            score += 15
        if has_conclusion:
            score += 15
        
        # Densidad de keywords
        if 1 <= keyword_density <= 3:
            score += 25
        elif keyword_density > 0:
            score += 10
        
        return min(score, 100)
    
    def _identify_content_gaps(self, content: str) -> List[str]:
        """Identifica gaps en el contenido"""
        gaps = []
        
        # Verificar elementos comunes
        if "ejemplo" not in content.lower() and "case study" not in content.lower():
            gaps.append("Falta ejemplos o casos de estudio")
        
        if "estad√≠stica" not in content.lower() and "%" not in content and "datos" not in content.lower():
            gaps.append("Falta datos estad√≠sticos o m√©tricas")
        
        if len(re.findall(r'https?://\S+', content)) == 0:
            gaps.append("Falta enlaces externos o referencias")
        
        if "futuro" not in content.lower() and "tendencia" not in content.lower():
            gaps.append("Falta perspectiva futura o tendencias")
        
        if "recomendaci√≥n" not in content.lower() and "acci√≥n" not in content.lower():
            gaps.append("Falta recomendaciones accionables")
        
        return gaps
    
    def _generate_recommendations(self, content: str) -> List[str]:
        """Genera recomendaciones para mejorar el contenido"""
        recommendations = []
        
        word_count = len(content.split())
        headers = len(re.findall(r'^#+\s+(.+)$', content, re.MULTILINE))
        
        if word_count < 1000:
            recommendations.append("Expandir el contenido: m√≠nimo 1000 palabras para SEO")
        
        if headers < 3:
            recommendations.append("Agregar m√°s subt√≠tulos para mejorar estructura")
        
        if len(re.findall(r'\*\*(.+?)\*\*', content)) == 0:
            recommendations.append("Usar texto en negrita para destacar puntos clave")
        
        if len(re.findall(r'^\s*[-*‚Ä¢]\s+', content, re.MULTILINE)) == 0:
            recommendations.append("Agregar listas con vi√±etas para mejor lectura")
        
        gaps = self._identify_content_gaps(content)
        if gaps:
            recommendations.extend([f"Mejorar: {gap}" for gap in gaps[:3]])
        
        return recommendations
    
    def _format_analysis_results(self, results: Dict[str, Any]) -> str:
        """Formatea los resultados del an√°lisis"""
        output = ["=== AN√ÅLISIS COMPLETO DE CONTENIDO ===\n"]
        
        # M√©tricas b√°sicas
        metrics = results["content_metrics"]
        output.append(f"üìä M√âTRICAS B√ÅSICAS:")
        output.append(f"   ‚Ä¢ Palabras: {metrics['word_count']}")
        output.append(f"   ‚Ä¢ Oraciones: {metrics['sentence_count']}")
        output.append(f"   ‚Ä¢ P√°rrafos: {metrics['paragraph_count']}")
        output.append(f"   ‚Ä¢ Promedio palabras/oraci√≥n: {metrics['average_words_per_sentence']}")
        
        # Keywords
        keywords = results["keyword_analysis"]
        output.append(f"\nüîç AN√ÅLISIS DE KEYWORDS:")
        output.append(f"   ‚Ä¢ Palabras √∫nicas: {keywords['total_unique_words']}")
        output.append(f"   ‚Ä¢ Densidad de keywords: {keywords['keyword_density']}%")
        if keywords['marketing_keywords_found']:
            output.append("   ‚Ä¢ Keywords de marketing encontradas:")
            for kw, count in list(keywords['marketing_keywords_found'].items())[:5]:
                output.append(f"     - {kw}: {count} veces")
        
        # Estructura
        structure = results["structure_analysis"]
        output.append(f"\nüèóÔ∏è ESTRUCTURA:")
        output.append(f"   ‚Ä¢ Headers: {structure['headers_count']}")
        output.append(f"   ‚Ä¢ Listas con vi√±etas: {structure['bullet_points']}")
        output.append(f"   ‚Ä¢ Enlaces: {structure['links_count']}")
        output.append(f"   ‚Ä¢ Buena estructura: {'‚úÖ' if structure['has_good_structure'] else '‚ùå'}")
        
        # Legibilidad
        readability = results["readability"]
        if "error" not in readability:
            output.append(f"\nüìñ LEGIBILIDAD:")
            output.append(f"   ‚Ä¢ Score Flesch: {readability['flesch_score']}")
            output.append(f"   ‚Ä¢ Nivel: {readability['reading_level']}")
            output.append(f"   ‚Ä¢ Tiempo estimado lectura: {readability['estimated_reading_time_minutes']} min")
        
        # SEO
        seo = results["seo_metrics"]
        output.append(f"\nüéØ M√âTRICAS SEO:")
        output.append(f"   ‚Ä¢ Longitud √≥ptima: {'‚úÖ' if seo['optimal_length'] else '‚ùå'}")
        output.append(f"   ‚Ä¢ Score SEO: {seo['seo_score']}/100")
        output.append(f"   ‚Ä¢ Densidad keywords: {seo['keyword_density']}%")
        
        # Gaps y recomendaciones
        gaps = results["content_gaps"]
        if gaps:
            output.append(f"\n‚ö†Ô∏è GAPS IDENTIFICADOS:")
            for gap in gaps:
                output.append(f"   ‚Ä¢ {gap}")
        
        recommendations = results["recommendations"]
        if recommendations:
            output.append(f"\nüí° RECOMENDACIONES:")
            for rec in recommendations:
                output.append(f"   ‚Ä¢ {rec}")
        
        return "\n".join(output)
    
    def _analyze_keywords(self, content: str) -> str:
        """An√°lisis espec√≠fico de keywords"""
        keywords = self._extract_keywords(content)
        return json.dumps(keywords, indent=2)
    
    def _analyze_readability(self, content: str) -> str:
        """An√°lisis espec√≠fico de legibilidad"""
        readability = self._calculate_readability(content)
        return json.dumps(readability, indent=2)
    
    def _analyze_structure(self, content: str) -> str:
        """An√°lisis espec√≠fico de estructura"""
        structure = self._analyze_content_structure(content)
        return json.dumps(structure, indent=2)
    
    def _analyze_seo(self, content: str) -> str:
        """An√°lisis espec√≠fico de SEO"""
        seo = self._analyze_seo_factors(content)
        return json.dumps(seo, indent=2)